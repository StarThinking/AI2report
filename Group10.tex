\documentclass[11pt]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{breakurl} 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{mathptmx}
\def \hfillx {\hspace*{-\textwidth} \hfill}

\title{Hand-written Digits Classification and Letter Recognition}
\author{Group 10 \\ Sixiang Ma, Yuan Xiao, Xiaokuan Zhang}
\date{12/06/2016}



\begin{document}

\maketitle
\section*{Abstract}
say something

\section{Introduction}

\subsection{Problem Description}
Our group wants to solve the problem of classifying hand-written digits and  black-and-white rectangular pixel-displayed letters. We have two datasets from UCI machine learning repository \cite{Lichman2013}. One is hand-written digits dataset \cite{digitdataset}. It has 5620 instances. For each instance, there are 1024 attributes (32x32 matrix), whose values are either 0 or 1. After grouping every 4x4 blocks, the dimension is reduced to 64 (8x8 matrix), and each attribute ranges from 0 to 16. Another is letter recognition dataset \cite{letterdataset}. The character images were based on 20 different fonts. It has 20000 instances. For each instance, there are 16 attributes, whose values range from 0 to 15. More details can be found by visiting the links.

\subsection{Classifying Algorithms}
In this report, we test three machine learning  algorithms, i.e., Support Vector Machine (SVM), Neural Networks, and Naive Bayes. We will present the detail of algorithms in Section \ref{sec:metho}.
\subsection{Result Summary}
For the first dataset, after tuning the parameters, the three classifiers can achieve , , , accuracy. 

\section{Background}
The hand-written digit dataset was first used in \cite{kaynak1995methods}. In the paper they combined different classifiers to obtain a better performance.  The letter recognition dataset was first used in \cite{frey1991letter}. They generated classification rules to distinguish different letters.

\section{Methodology}\label{sec:metho}

\subsection{Support Vector Machine}
Support vector machines (SVMs) \cite{cortes1995support} are supervised learning models that analyze data for classicifation or regression. Given a set of training examples, which are marked with their belonging categories, a SVM algorithm builds a model to recognize and assign testing examples to the predicted categories. 

(Say something about kernel functions).

\subsection{Neural Networks}
Neural networks, or artificial neural networks \cite{hagan1996neural}, simulate the functions of nerve cells of human brain and serve as an important computational approach in machine learning. They typically form a structure of multiple layers of basic perceptrons and support both supervised and unsupervised learning.

Neural networks have a long history, dating back to the 1940s \cite{mcculloch1943logical}. However, the idea of artificial neural networks was not popular at early days due to its limitation in solving logical calculations \cite{minsky1988perceptrons}. Modern neural networks revived in the past decade, along with the rise of deep learning \cite{bengio2009learning, schmidhuber2015deep}.

\subsection{Naive Bayes}
Naive Bayes classifier makes use of the Bayes Theorem. It is basically a conditional probability model. It is one of the simplest machine learning algorithms. Compared to Bayesian Networks, Naive Bayes is technically a special case by assuming that all features are conditionally independent from each other given the class label. One of the earliest papers that described this algorithm was from 1970s \cite{duda1973pattern}. 

\section{Evaluation}
In this project, we mainly used \texttt{Weka} \cite{hall2009weka} to test different machine learning algorithms.

\subsection{Hand-written Digits Classification}
\subsubsection{Support Vector Machine}

\subsubsection{Neural Networks}

\subsubsection{Naive Bayes}

\subsection{Letter Recognition}
\subsubsection{Support Vector Machine}

\subsubsection{Neural Networks}

\subsubsection{Naive Bayes}

\section{Discussion}
\subsection{Evaluation Matrix}
In our presentation, we used \texttt{root relative squared error} to evaluate the performance of our algorithms and falsely claimed that SVM was not suitable for our datasets. However, it makes little sense to evaluate this feature on  non-binary datasets. So in the report, we changed it to F-measure.

\subsection{Different Kernels of SVM}
In our presentation, we only talked about the polynomial kernel. In the report, we added xxx, xxx kernels to compare the performance. By comparison, we can see that ......
 
\section{Conclusion}



\newpage
\bibliographystyle{plain}
\bibliography{Group10}
\end{document}







