\documentclass[11pt]{article}

\usepackage{fullpage} % Package to use full page
\usepackage{parskip} % Package to tweak paragraph skipping
\usepackage{tikz} % Package for drawing
\usepackage{amsmath}
\usepackage[breaklinks]{hyperref}
\usepackage{url}
\usepackage{breakurl} 
\usepackage{enumerate}
\usepackage{subcaption}
\usepackage{mathptmx}
\usepackage{epsfig}
\usepackage{multirow}
\def \hfillx {\hspace*{-\textwidth} \hfill}

\title{Hand-written Digits Classification and Letter Recognition}
\author{Group 10 \\ Sixiang Ma, Yuan Xiao, Xiaokuan Zhang}
\date{12/09/2016}



\begin{document}

\maketitle
\section*{Abstract}
say something

\section{Introduction}

\subsection{Problem Description}
Our group wants to solve the problem of classifying hand-written digits and  black-and-white rectangular pixel-displayed letters. We have two datasets from UCI machine learning repository \cite{Lichman2013}. One is hand-written digits dataset \cite{digitdataset}. It has 5620 instances. For each instance, there are 1024 attributes (32x32 matrix), whose values are either 0 or 1. After grouping every 4x4 blocks, the dimension is reduced to 64 (8x8 matrix), and each attribute ranges from 0 to 16. Another is letter recognition dataset \cite{letterdataset}. The character images were based on 20 different fonts. It has 20000 instances. For each instance, there are 16 attributes, whose values range from 0 to 15. More details can be found by visiting the links.

\subsection{Classifying Algorithms}
In this report, we test three machine learning  algorithms, i.e., Support Vector Machine (SVM), Neural Networks, and Naive Bayes. We will present the detail of algorithms in Section \ref{sec:metho}.
\subsection{Result Summary}
For the first dataset, after tuning the parameters, the three classifiers can achieve , , , accuracy. 

\section{Background}
The hand-written digit dataset was first used in \cite{kaynak1995methods}. In the paper they combined different classifiers to obtain a better performance.  The letter recognition dataset was first used in \cite{frey1991letter}. They generated classification rules to distinguish different letters.

\section{Methodology}\label{sec:metho}

\subsection{Support Vector Machine}
Support vector machines (SVMs) \cite{cortes1995support} are supervised learning models that analyze data for classicifation or regression. Given a set of training examples, which are marked with their belonging categories, a SVM algorithm builds a model to recognize and assign testing examples to the predicted categories. 

(Say something about kernel functions).

\subsection{Neural Networks}
Neural networks, or artificial neural networks \cite{hagan1996neural}, simulate the functions of nerve cells of human brain and serve as an important computational approach in machine learning. They typically form a structure of multiple layers of basic perceptrons and support both supervised and unsupervised learning.

Neural networks have a long history, dating back to the 1940s \cite{mcculloch1943logical}. However, the idea of artificial neural networks was not popular at early days due to its limitation in solving logical calculations \cite{minsky1988perceptrons}. Modern neural networks revived in the past decade, along with the rise of deep learning \cite{bengio2009learning, schmidhuber2015deep}.

\subsection{Naive Bayes}
Naive Bayes classifier makes use of the Bayes Theorem. It is basically a conditional probability model. It is one of the simplest machine learning algorithms. Compared to Bayesian Networks, Naive Bayes is technically a special case by assuming that all features are conditionally independent from each other given the class label. One of the earliest papers that described this algorithm was from 1970s \cite{duda1973pattern}. 

\section{Evaluation}
In this project, we mainly used \texttt{Weka} \cite{hall2009weka} to test different machine learning algorithms. For each algorithm, we use different parameter settings, which will be covered in this section.

\subsection{Hand-written Digits Classification}
\subsubsection{Support Vector Machine}

\subsubsection{Neural Networks}

\subsubsection{Naive Bayes}
The Naive Bayes works well on the Hand-written Digit dataset. In Figure \ref{fig:bayes}, K means whether to use kernel estimator, and D means whether to use supervised discretization. In Figure \ref{fig:digit-bayes}, when K=F, D=F, which is the worst case scenario, the accuracy and F-measure are still higher than 90\%. In the best case (K=T, D=F), they both exceeds 92\%.


\subsection{Letter Recognition}
\subsubsection{Support Vector Machine}

\subsubsection{Neural Networks}

\subsubsection{Naive Bayes}
Naive Bayes does not work well on the Letter Recognition dataset. From Figure \ref{fig:letter-bayes}, we can see that the highest accuracy is lower than 75\%. Generally speaking, Naive Bayes is not suitable for classifying letters.

In this dataset, we are also interested in which letters Naive Bayes performs worst. Table \ref{tbl:bayes} shows the top 3 worst cases when using Naive Bayes with different parameters. In all three parameter settings, `H' is always one of the top 3, which means that `H' is quite hard to classify for Naive Bayes. Also, the same applies to `S' and `X', as they appear in two of the three cases.



\begin{table}[!htb]
%\begin{subtable}

\centering
\begin{tabular}{c  c  c} \hline
% centering table
% creating 10 columns
% inserting double-line 

Parameters & Class (Letters) & Accuracy \\\hline
\multirow{3}{*}{K = F, D = F} & S & 29.4\% \\
	& H & 30.5\% \\
	& Y & 33.1\% \\\hline
\multirow{3}{*}{K = T, D = F} & H & 57.4\% \\
	& S & 64.2\% \\
	& X & 64.3\% \\\hline
\multirow{3}{*}{K = F, D = T} & H & 57.5\% \\
	& E & 60.7\% \\
	& X & 64.4\% \\\hline
\end{tabular}
\caption{Worst Cases: Naive Bayes} % title name of the table
\label{tbl:bayes}
%\end{subtable} 
\end{table}


\begin{figure}[htbp]
\centering

\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/digit_bayes}
\caption{Digit Classification}
\label{fig:digit-bayes}
\end{subfigure}
\hfill
\begin{subfigure}[htbp]{0.46\columnwidth}
\includegraphics*[width=\textwidth]{fig/letter_bayes}
\caption{Letter Recognition}
\label{fig:letter-bayes}
\end{subfigure}
\caption{Performance of Naive Bayes}
\label{fig:bayes}
\end{figure}
\section{Discussion}

\subsection{Evaluation Matrix}
In our presentation, we used \texttt{root relative squared error} to evaluate the performance of our algorithms and falsely claimed that SVM was not suitable for our datasets. However, it makes little sense to evaluate this feature on  non-binary datasets. So in the report, we changed it to F-measure.

\subsection{Different Kernels of SVM}
In our presentation, we only talked about the polynomial kernel. In the report, we added xxx, xxx kernels to compare the performance. By comparison, we can see that ......


\subsection{Parameters of Multi-layer Perceptron}
 
\section{Conclusion}



\newpage
\bibliographystyle{plain}
\bibliography{Group10}
\end{document}







